{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import OneHotCategorical, Normal\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sub_models.functions_losses import SymLogTwoHotLoss\n",
    "\n",
    "from sub_models.attention_blocks import get_subsequent_mask_with_batch_length, get_subsequent_mask\n",
    "from sub_models.transformer_model import StochasticTransformerKVCache\n",
    "from sub_models.attention_blocks import get_vector_mask\n",
    "from sub_models.attention_blocks import PositionalEncoding1D, AttentionBlock, AttentionBlockKVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTransformerKVCache2(nn.Module):\n",
    "    def __init__(self, stoch_dim, action_dim, feat_dim, num_layers, num_heads, max_length, dropout):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        # mix image_embedding and action\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Linear(stoch_dim+action_dim, feat_dim, bias=False),\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feat_dim, feat_dim, bias=False),\n",
    "            nn.LayerNorm(feat_dim)\n",
    "        )\n",
    "        self.position_encoding = PositionalEncoding1D(max_length=max_length, embed_dim=feat_dim)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            AttentionBlockKVCache(feat_dim=feat_dim, hidden_dim=feat_dim*2, num_heads=num_heads, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(feat_dim, eps=1e-6)  # TODO: check if this is necessary\n",
    "\n",
    "    def forward(self, samples, action, mask):\n",
    "        '''\n",
    "        Normal forward pass\n",
    "        '''\n",
    "        # action is not one hot\n",
    "        # action = F.one_hot(action.long(), self.action_dim).float() \n",
    "        print(mask)\n",
    "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
    "        feats = self.position_encoding(feats)\n",
    "        feats = self.layer_norm(feats)\n",
    "        for layer in self.layer_stack:\n",
    "            print(\"pre:\",feats)\n",
    "            feats, attn = layer(feats, feats, feats, mask)\n",
    "            print(\"post:\",feats)\n",
    "        return feats\n",
    "\n",
    "    def reset_kv_cache_list(self, batch_size, dtype):\n",
    "        '''\n",
    "        Reset self.kv_cache_list\n",
    "        '''\n",
    "        self.kv_cache_list = []\n",
    "        for layer in self.layer_stack:\n",
    "            self.kv_cache_list.append(torch.zeros(size=(batch_size, 0, self.feat_dim), dtype=dtype, device=\"cpu\"))\n",
    "            \n",
    "\n",
    "    def forward_with_kv_cache(self, samples, action,test):\n",
    "        '''\n",
    "        Forward pass with kv_cache, cache stored in self.kv_cache_list\n",
    "        '''\n",
    "        assert samples.shape[1] == 1\n",
    "        mask = get_vector_mask(self.kv_cache_list[0].shape[1]+1, samples.device)\n",
    "        # print(mask)\n",
    "        # action = F.one_hot(action.long(), self.action_dim).float()\n",
    "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
    "        print(\"feats1.dim\",feats.shape)\n",
    "        feats = self.position_encoding.forward_with_position(feats, position=self.kv_cache_list[0].shape[1])\n",
    "        print(\"feats2.dim\",feats.shape)\n",
    "        feats = self.layer_norm(feats)\n",
    "        print(\"feats3.dim\",feats.shape)\n",
    "        for idx, layer in enumerate(self.layer_stack):\n",
    "            self.kv_cache_list[idx] = torch.cat([self.kv_cache_list[idx], feats], dim=1)\n",
    "            # print(\"pre:\",self.kv_cache_list[idx])\n",
    "            print(\"feats4.dim\",feats.shape)\n",
    "            feats, attn = layer(feats, self.kv_cache_list[idx], self.kv_cache_list[idx], mask)\n",
    "            # print(\"post:\",feats)\n",
    "\n",
    "        return feats\n",
    "\n",
    "    def forward_context(self, samples, action, mask):\n",
    "        '''\n",
    "        Normal forward pass\n",
    "        '''\n",
    "        # action is not one hot\n",
    "        # action = F.one_hot(action.long(), self.action_dim).float() \n",
    "        # print(mask)\n",
    "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
    "        feats = self.position_encoding(feats)\n",
    "        feats = self.layer_norm(feats)\n",
    "        kv_cache_test = feats\n",
    "        for layer in self.layer_stack:\n",
    "            # print(\"pre:\",feats)\n",
    "            feats, attn = layer(feats, feats, feats, mask)\n",
    "            # print(\"post:\",feats)\n",
    "        return feats,kv_cache_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_transformer = StochasticTransformerKVCache2(\n",
    "            stoch_dim=3,\n",
    "            action_dim=1,\n",
    "            feat_dim=3,\n",
    "            num_layers=1,\n",
    "            num_heads=1,\n",
    "            max_length=5,\n",
    "            dropout=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "latent = torch.rand(4,5,3)\n",
    "action = torch.rand(4,5,1)\n",
    "temporal_mask = get_subsequent_mask(latent)\n",
    "print(temporal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     dist_feat = storm_transformer(latent, action, temporal_mask)\n",
    "storm_transformer.eval()\n",
    "dist_feat, kv_test = storm_transformer.forward_context(latent, action, temporal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats1.dim torch.Size([4, 1, 3])\n",
      "feats2.dim torch.Size([4, 1, 3])\n",
      "feats3.dim torch.Size([4, 1, 3])\n",
      "feats4.dim torch.Size([4, 1, 3])\n",
      "feats1.dim torch.Size([4, 1, 3])\n",
      "feats2.dim torch.Size([4, 1, 3])\n",
      "feats3.dim torch.Size([4, 1, 3])\n",
      "feats4.dim torch.Size([4, 1, 3])\n",
      "feats1.dim torch.Size([4, 1, 3])\n",
      "feats2.dim torch.Size([4, 1, 3])\n",
      "feats3.dim torch.Size([4, 1, 3])\n",
      "feats4.dim torch.Size([4, 1, 3])\n",
      "feats1.dim torch.Size([4, 1, 3])\n",
      "feats2.dim torch.Size([4, 1, 3])\n",
      "feats3.dim torch.Size([4, 1, 3])\n",
      "feats4.dim torch.Size([4, 1, 3])\n",
      "feats1.dim torch.Size([4, 1, 3])\n",
      "feats2.dim torch.Size([4, 1, 3])\n",
      "feats3.dim torch.Size([4, 1, 3])\n",
      "feats4.dim torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# with torch.inference_mode():\n",
    "#     storm_transformer.reset_kv_cache_list(4,torch.float32)\n",
    "#     test_list = []\n",
    "#     for i in range(3):\n",
    "#         dist_feat_kv = storm_transformer.forward_with_kv_cache(latent[:,i:i+1,:], action[:,i:i+1,:],i)\n",
    "#         test_list.append(dist_feat_kv)\n",
    "storm_transformer.reset_kv_cache_list(4,torch.float32)\n",
    "test_list = []\n",
    "for i in range(5):\n",
    "    dist_feat_kv = storm_transformer.forward_with_kv_cache(latent[:,i:i+1,:], action[:,i:i+1,:],i)\n",
    "    test_list.append(dist_feat_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3618,  1.0031, -1.3649],\n",
       "         [-1.3932,  0.9071,  0.4861],\n",
       "         [-1.3437,  0.2900,  1.0537],\n",
       "         [-0.2231,  1.3210, -1.0978],\n",
       "         [-1.4036,  0.8514,  0.5522]],\n",
       "\n",
       "        [[-0.6642,  1.4134, -0.7492],\n",
       "         [-1.4035,  0.8524,  0.5510],\n",
       "         [-1.2966,  0.1593,  1.1373],\n",
       "         [-1.4082,  0.8174,  0.5908],\n",
       "         [-1.1769,  1.2676, -0.0907]],\n",
       "\n",
       "        [[ 0.1203,  1.1602, -1.2804],\n",
       "         [-1.1619,  1.2792, -0.1173],\n",
       "         [-1.4012,  0.5349,  0.8663],\n",
       "         [-0.6396,  1.4121, -0.7725],\n",
       "         [-1.3687,  0.9926,  0.3760]],\n",
       "\n",
       "        [[ 0.2109,  1.1056, -1.3165],\n",
       "         [-1.1716,  1.2717, -0.1001],\n",
       "         [-1.3078,  0.1879,  1.1199],\n",
       "         [-1.3728,  0.9805,  0.3924],\n",
       "         [-1.3350,  1.0716,  0.2634]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_feat[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3618,  1.0031, -1.3649],\n",
       "         [-1.3932,  0.9071,  0.4861],\n",
       "         [-1.3437,  0.2900,  1.0537],\n",
       "         [-0.2231,  1.3210, -1.0978],\n",
       "         [-1.4036,  0.8514,  0.5522]],\n",
       "\n",
       "        [[-0.6642,  1.4134, -0.7492],\n",
       "         [-1.4035,  0.8524,  0.5510],\n",
       "         [-1.2966,  0.1593,  1.1373],\n",
       "         [-1.4082,  0.8174,  0.5908],\n",
       "         [-1.1769,  1.2676, -0.0907]],\n",
       "\n",
       "        [[ 0.1203,  1.1602, -1.2804],\n",
       "         [-1.1619,  1.2792, -0.1173],\n",
       "         [-1.4012,  0.5349,  0.8663],\n",
       "         [-0.6396,  1.4121, -0.7725],\n",
       "         [-1.3687,  0.9926,  0.3760]],\n",
       "\n",
       "        [[ 0.2109,  1.1056, -1.3165],\n",
       "         [-1.1716,  1.2717, -0.1001],\n",
       "         [-1.3078,  0.1879,  1.1199],\n",
       "         [-1.3728,  0.9805,  0.3924],\n",
       "         [-1.3350,  1.0716,  0.2634]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.cat(test_list,dim=1)\n",
    "result[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.0911,  1.1767, -1.2677],\n",
       "          [-1.2728,  1.1702,  0.1027],\n",
       "          [-1.4135,  0.7461,  0.6673],\n",
       "          [-0.3324,  1.3566, -1.0242],\n",
       "          [-1.2743,  1.1683,  0.1059]],\n",
       " \n",
       "         [[-0.6688,  1.4135, -0.7447],\n",
       "          [-1.2728,  1.1702,  0.1027],\n",
       "          [-1.4117,  0.6334,  0.7784],\n",
       "          [-1.2633,  1.1821,  0.0812],\n",
       "          [-0.9749,  1.3747, -0.3997]],\n",
       " \n",
       "         [[-0.1154,  1.2784, -1.1630],\n",
       "          [-1.0452,  1.3476, -0.3024],\n",
       "          [-1.3808,  0.9552,  0.4256],\n",
       "          [-0.6298,  1.4115, -0.7817],\n",
       "          [-1.2110,  1.2380, -0.0270]],\n",
       " \n",
       "         [[-0.0415,  1.2450, -1.2035],\n",
       "          [-1.0557,  1.3428, -0.2871],\n",
       "          [-1.4117,  0.6334,  0.7784],\n",
       "          [-1.2049,  1.2436, -0.0387],\n",
       "          [-1.1499,  1.2879, -0.1380]]], grad_fn=<CatBackward0>)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_transformer.kv_cache_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3687, -0.3761, -0.9926],\n",
       "         [ 1.2716, -0.0997, -1.1718],\n",
       "         [ 1.1253,  0.1792, -1.3045],\n",
       "         [ 1.3443, -0.2919, -1.0524],\n",
       "         [ 0.2353,  1.0900, -1.3253]],\n",
       "\n",
       "        [[ 1.3525, -0.3185, -1.0341],\n",
       "         [ 1.2684, -0.0925, -1.1759],\n",
       "         [ 1.1253,  0.1792, -1.3045],\n",
       "         [ 1.3443, -0.2919, -1.0524],\n",
       "         [ 0.2310,  1.0928, -1.3238]],\n",
       "\n",
       "        [[ 1.3687, -0.3761, -0.9926],\n",
       "         [ 1.2622, -0.0786, -1.1835],\n",
       "         [ 1.1253,  0.1792, -1.3045],\n",
       "         [ 1.3443, -0.2919, -1.0524],\n",
       "         [ 0.2310,  1.0928, -1.3238]],\n",
       "\n",
       "        [[ 1.3687, -0.3761, -0.9926],\n",
       "         [ 1.2716, -0.0997, -1.1718],\n",
       "         [ 1.1253,  0.1792, -1.3045],\n",
       "         [ 1.3443, -0.2919, -1.0524],\n",
       "         [ 0.1941,  1.1161, -1.3102]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_test[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unitree-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
