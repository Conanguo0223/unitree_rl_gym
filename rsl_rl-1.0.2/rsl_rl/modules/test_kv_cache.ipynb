{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import OneHotCategorical, Normal\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sub_models.functions_losses import SymLogTwoHotLoss\n",
    "\n",
    "from sub_models.attention_blocks import get_subsequent_mask_with_batch_length, get_subsequent_mask\n",
    "from sub_models.transformer_model import StochasticTransformerKVCache\n",
    "from sub_models.attention_blocks import get_vector_mask\n",
    "from sub_models.attention_blocks import PositionalEncoding1D, AttentionBlock, AttentionBlockKVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTransformerKVCache2(nn.Module):\n",
    "    def __init__(self, stoch_dim, action_dim, feat_dim, num_layers, num_heads, max_length, dropout):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        # mix image_embedding and action\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Linear(stoch_dim+action_dim, feat_dim, bias=False),\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feat_dim, feat_dim, bias=False),\n",
    "            nn.LayerNorm(feat_dim)\n",
    "        )\n",
    "        self.position_encoding = PositionalEncoding1D(max_length=max_length, embed_dim=feat_dim)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            AttentionBlockKVCache(feat_dim=feat_dim, hidden_dim=feat_dim*2, num_heads=num_heads, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(feat_dim, eps=1e-6)  # TODO: check if this is necessary\n",
    "\n",
    "    def forward(self, samples, action, mask):\n",
    "        '''\n",
    "        Normal forward pass\n",
    "        '''\n",
    "        # action is not one hot\n",
    "        # action = F.one_hot(action.long(), self.action_dim).float() \n",
    "        print(mask)\n",
    "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
    "        feats = self.position_encoding(feats)\n",
    "        feats = self.layer_norm(feats)\n",
    "        for layer in self.layer_stack:\n",
    "            print(\"pre:\",feats)\n",
    "            feats, attn = layer(feats, feats, feats, mask)\n",
    "            print(\"post:\",feats)\n",
    "        return feats\n",
    "\n",
    "    def reset_kv_cache_list(self, batch_size, dtype):\n",
    "        '''\n",
    "        Reset self.kv_cache_list\n",
    "        '''\n",
    "        self.kv_cache_list = []\n",
    "        for layer in self.layer_stack:\n",
    "            self.kv_cache_list.append(torch.zeros(size=(batch_size, 0, self.feat_dim), dtype=dtype, device=\"cpu\"))\n",
    "            \n",
    "\n",
    "    def forward_with_kv_cache(self, samples, action,test):\n",
    "        '''\n",
    "        Forward pass with kv_cache, cache stored in self.kv_cache_list\n",
    "        '''\n",
    "        assert samples.shape[1] == 1\n",
    "        mask = get_vector_mask(self.kv_cache_list[0].shape[1]+1, samples.device)\n",
    "        # print(mask)\n",
    "        # action = F.one_hot(action.long(), self.action_dim).float()\n",
    "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
    "        feats = self.position_encoding.forward_with_position(feats, position=self.kv_cache_list[0].shape[1])\n",
    "        feats = self.layer_norm(feats)\n",
    "        for idx, layer in enumerate(self.layer_stack):\n",
    "            self.kv_cache_list[idx] = torch.cat([self.kv_cache_list[idx], feats], dim=1)\n",
    "            print(\"pre:\",self.kv_cache_list[idx])\n",
    "            print(\"pres\",feats)\n",
    "            feats, attn = layer(feats, self.kv_cache_list[idx], self.kv_cache_list[idx], mask)\n",
    "            print(\"post:\",feats)\n",
    "\n",
    "        return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_transformer = StochasticTransformerKVCache2(\n",
    "            stoch_dim=3,\n",
    "            action_dim=1,\n",
    "            feat_dim=3,\n",
    "            num_layers=1,\n",
    "            num_heads=1,\n",
    "            max_length=3,\n",
    "            dropout=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "latent = torch.rand(4,3,3)\n",
    "action = torch.rand(4,3,1)\n",
    "temporal_mask = get_subsequent_mask(latent)\n",
    "print(temporal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True,  True]]])\n",
      "pre: tensor([[[ 1.1645,  0.1126, -1.2772],\n",
      "         [ 0.2367, -1.3258,  1.0891],\n",
      "         [ 1.1531, -1.2856,  0.1325]],\n",
      "\n",
      "        [[ 1.1580,  0.1241, -1.2821],\n",
      "         [-0.3539, -1.0088,  1.3627],\n",
      "         [ 1.2158, -1.2335,  0.0177]],\n",
      "\n",
      "        [[-0.5184,  1.3987, -0.8803],\n",
      "         [-0.0443, -1.2020,  1.2463],\n",
      "         [ 0.9855, -1.3711,  0.3856]],\n",
      "\n",
      "        [[ 1.1651,  0.1117, -1.2768],\n",
      "         [-0.6524, -0.7605,  1.4128],\n",
      "         [ 0.9855, -1.3711,  0.3856]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "post: tensor([[[ 1.0269,  0.3286, -1.3555],\n",
      "         [ 0.2449, -1.3287,  1.0838],\n",
      "         [ 1.2687, -1.1754, -0.0933]],\n",
      "\n",
      "        [[ 1.0212,  0.3366, -1.3579],\n",
      "         [-0.2319, -1.0922,  1.3241],\n",
      "         [ 1.3218, -1.0963, -0.2256]],\n",
      "\n",
      "        [[-0.6127,  1.4102, -0.7975],\n",
      "         [ 0.0307, -1.2398,  1.2091],\n",
      "         [ 1.0552, -1.3430,  0.2879]],\n",
      "\n",
      "        [[ 1.0274,  0.3280, -1.3553],\n",
      "         [-0.4938, -0.9007,  1.3946],\n",
      "         [ 1.0624, -1.3396,  0.2772]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# with torch.inference_mode():\n",
    "#     dist_feat = storm_transformer(latent, action, temporal_mask)\n",
    "storm_transformer.eval()\n",
    "dist_feat = storm_transformer(latent, action, temporal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: tensor([[[ 1.1645,  0.1126, -1.2772]],\n",
      "\n",
      "        [[ 1.1580,  0.1241, -1.2821]],\n",
      "\n",
      "        [[-0.5184,  1.3987, -0.8803]],\n",
      "\n",
      "        [[ 1.1651,  0.1117, -1.2768]]], grad_fn=<CatBackward0>)\n",
      "pres tensor([[[ 1.1645,  0.1126, -1.2772]],\n",
      "\n",
      "        [[ 1.1580,  0.1241, -1.2821]],\n",
      "\n",
      "        [[-0.5184,  1.3987, -0.8803]],\n",
      "\n",
      "        [[ 1.1651,  0.1117, -1.2768]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "post: tensor([[[ 1.0269,  0.3286, -1.3555]],\n",
      "\n",
      "        [[ 1.0212,  0.3366, -1.3579]],\n",
      "\n",
      "        [[-0.6127,  1.4102, -0.7975]],\n",
      "\n",
      "        [[ 1.0274,  0.3280, -1.3553]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "pre: tensor([[[ 1.1645,  0.1126, -1.2772],\n",
      "         [ 0.2367, -1.3258,  1.0891]],\n",
      "\n",
      "        [[ 1.1580,  0.1241, -1.2821],\n",
      "         [-0.3539, -1.0088,  1.3627]],\n",
      "\n",
      "        [[-0.5184,  1.3987, -0.8803],\n",
      "         [-0.0443, -1.2020,  1.2463]],\n",
      "\n",
      "        [[ 1.1651,  0.1117, -1.2768],\n",
      "         [-0.6524, -0.7605,  1.4128]]], grad_fn=<CatBackward0>)\n",
      "pres tensor([[[ 0.2367, -1.3258,  1.0891]],\n",
      "\n",
      "        [[-0.3539, -1.0088,  1.3627]],\n",
      "\n",
      "        [[-0.0443, -1.2020,  1.2463]],\n",
      "\n",
      "        [[-0.6524, -0.7605,  1.4128]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "post: tensor([[[ 0.2449, -1.3287,  1.0838]],\n",
      "\n",
      "        [[-0.2319, -1.0922,  1.3241]],\n",
      "\n",
      "        [[ 0.0307, -1.2398,  1.2091]],\n",
      "\n",
      "        [[-0.4938, -0.9007,  1.3946]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "pre: tensor([[[ 1.1645,  0.1126, -1.2772],\n",
      "         [ 0.2367, -1.3258,  1.0891],\n",
      "         [ 1.1531, -1.2856,  0.1325]],\n",
      "\n",
      "        [[ 1.1580,  0.1241, -1.2821],\n",
      "         [-0.3539, -1.0088,  1.3627],\n",
      "         [ 1.2158, -1.2335,  0.0177]],\n",
      "\n",
      "        [[-0.5184,  1.3987, -0.8803],\n",
      "         [-0.0443, -1.2020,  1.2463],\n",
      "         [ 0.9855, -1.3711,  0.3856]],\n",
      "\n",
      "        [[ 1.1651,  0.1117, -1.2768],\n",
      "         [-0.6524, -0.7605,  1.4128],\n",
      "         [ 0.9855, -1.3711,  0.3856]]], grad_fn=<CatBackward0>)\n",
      "pres tensor([[[ 1.1531, -1.2856,  0.1325]],\n",
      "\n",
      "        [[ 1.2158, -1.2335,  0.0177]],\n",
      "\n",
      "        [[ 0.9855, -1.3711,  0.3856]],\n",
      "\n",
      "        [[ 0.9855, -1.3711,  0.3856]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "post: tensor([[[ 1.2687, -1.1754, -0.0933]],\n",
      "\n",
      "        [[ 1.3218, -1.0963, -0.2256]],\n",
      "\n",
      "        [[ 1.0552, -1.3430,  0.2879]],\n",
      "\n",
      "        [[ 1.0624, -1.3396,  0.2772]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# with torch.inference_mode():\n",
    "#     storm_transformer.reset_kv_cache_list(4,torch.float32)\n",
    "#     test_list = []\n",
    "#     for i in range(3):\n",
    "#         dist_feat_kv = storm_transformer.forward_with_kv_cache(latent[:,i:i+1,:], action[:,i:i+1,:],i)\n",
    "#         test_list.append(dist_feat_kv)\n",
    "storm_transformer.reset_kv_cache_list(4,torch.float32)\n",
    "test_list = []\n",
    "for i in range(3):\n",
    "    dist_feat_kv = storm_transformer.forward_with_kv_cache(latent[:,i:i+1,:], action[:,i:i+1,:],i)\n",
    "    test_list.append(dist_feat_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0269,  0.3286, -1.3555],\n",
       "         [ 0.2449, -1.3287,  1.0838],\n",
       "         [ 1.2687, -1.1754, -0.0933]],\n",
       "\n",
       "        [[ 1.0212,  0.3366, -1.3579],\n",
       "         [-0.2319, -1.0922,  1.3241],\n",
       "         [ 1.3218, -1.0963, -0.2256]],\n",
       "\n",
       "        [[-0.6127,  1.4102, -0.7975],\n",
       "         [ 0.0307, -1.2398,  1.2091],\n",
       "         [ 1.0552, -1.3430,  0.2879]],\n",
       "\n",
       "        [[ 1.0274,  0.3280, -1.3553],\n",
       "         [-0.4938, -0.9007,  1.3946],\n",
       "         [ 1.0624, -1.3396,  0.2772]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_feat[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0269,  0.3286, -1.3555],\n",
       "         [ 0.2449, -1.3287,  1.0838],\n",
       "         [ 1.2687, -1.1754, -0.0933]],\n",
       "\n",
       "        [[ 1.0212,  0.3366, -1.3579],\n",
       "         [-0.2319, -1.0922,  1.3241],\n",
       "         [ 1.3218, -1.0963, -0.2256]],\n",
       "\n",
       "        [[-0.6127,  1.4102, -0.7975],\n",
       "         [ 0.0307, -1.2398,  1.2091],\n",
       "         [ 1.0552, -1.3430,  0.2879]],\n",
       "\n",
       "        [[ 1.0274,  0.3280, -1.3553],\n",
       "         [-0.4938, -0.9007,  1.3946],\n",
       "         [ 1.0624, -1.3396,  0.2772]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.cat(test_list,dim=1)\n",
    "result[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unitree-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
