{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import OneHotCategorical, Normal\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sub_models.functions_losses import SymLogTwoHotLoss\n",
    "\n",
    "from sub_models.attention_blocks import get_subsequent_mask_with_batch_length, get_subsequent_mask\n",
    "from sub_models.transformer_model import StochasticTransformerKVCache\n",
    "from sub_models.attention_blocks import get_vector_mask\n",
    "from sub_models.attention_blocks import PositionalEncoding1D, AttentionBlock, AttentionBlockKVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTransformerKVCache2(nn.Module):\n",
    "    def __init__(self, stoch_dim, action_dim, feat_dim, num_layers, num_heads, max_length, dropout):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        # mix image_embedding and action\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Linear(stoch_dim+action_dim, feat_dim, bias=False),\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feat_dim, feat_dim, bias=False),\n",
    "            nn.LayerNorm(feat_dim)\n",
    "        )\n",
    "        self.position_encoding = PositionalEncoding1D(max_length=max_length, embed_dim=feat_dim)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            AttentionBlockKVCache(feat_dim=feat_dim, hidden_dim=feat_dim*2, num_heads=num_heads, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(feat_dim, eps=1e-6)  # TODO: check if this is necessary\n",
    "\n",
    "    def forward(self, samples, action, mask):\n",
    "        '''\n",
    "        Normal forward pass\n",
    "        '''\n",
    "        # action is not one hot\n",
    "        # action = F.one_hot(action.long(), self.action_dim).float() \n",
    "        print(mask)\n",
    "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
    "        feats = self.position_encoding(feats)\n",
    "        feats = self.layer_norm(feats)\n",
    "        for layer in self.layer_stack:\n",
    "            print(\"pre:\",feats)\n",
    "            feats, attn = layer(feats, feats, feats, mask)\n",
    "            print(\"post:\",feats)\n",
    "        return feats\n",
    "\n",
    "    def reset_kv_cache_list(self, batch_size, dtype):\n",
    "        '''\n",
    "        Reset self.kv_cache_list\n",
    "        '''\n",
    "        self.kv_cache_list = []\n",
    "        for layer in self.layer_stack:\n",
    "            self.kv_cache_list.append(torch.zeros(size=(batch_size, 0, self.feat_dim), dtype=dtype, device=\"cpu\"))\n",
    "            \n",
    "\n",
    "    def forward_with_kv_cache(self, samples, action,test):\n",
    "        '''\n",
    "        Forward pass with kv_cache, cache stored in self.kv_cache_list\n",
    "        '''\n",
    "        assert samples.shape[1] == 1\n",
    "        mask = get_vector_mask(self.kv_cache_list[0].shape[1]+1, samples.device)\n",
    "        # print(mask)\n",
    "        # action = F.one_hot(action.long(), self.action_dim).float()\n",
    "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
    "        feats = self.position_encoding.forward_with_position(feats, position=self.kv_cache_list[0].shape[1])\n",
    "        feats = self.layer_norm(feats)\n",
    "        for idx, layer in enumerate(self.layer_stack):\n",
    "            self.kv_cache_list[idx] = torch.cat([self.kv_cache_list[idx], feats], dim=1)\n",
    "            # print(\"pre:\",self.kv_cache_list[idx])\n",
    "            # print(\"pres\",feats)\n",
    "            feats, attn = layer(feats, self.kv_cache_list[idx], self.kv_cache_list[idx], mask)\n",
    "            # print(\"post:\",feats)\n",
    "\n",
    "        return feats\n",
    "\n",
    "    def forward_context(self, samples, action, mask):\n",
    "        '''\n",
    "        Normal forward pass\n",
    "        '''\n",
    "        # action is not one hot\n",
    "        # action = F.one_hot(action.long(), self.action_dim).float() \n",
    "        # print(mask)\n",
    "        feats = self.stem(torch.cat([samples, action], dim=-1))\n",
    "        feats = self.position_encoding(feats)\n",
    "        feats = self.layer_norm(feats)\n",
    "        kv_cache_test = feats\n",
    "        for layer in self.layer_stack:\n",
    "            # print(\"pre:\",feats)\n",
    "            feats, attn = layer(feats, feats, feats, mask)\n",
    "            # print(\"post:\",feats)\n",
    "        return feats,kv_cache_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_transformer = StochasticTransformerKVCache2(\n",
    "            stoch_dim=3,\n",
    "            action_dim=1,\n",
    "            feat_dim=3,\n",
    "            num_layers=1,\n",
    "            num_heads=1,\n",
    "            max_length=5,\n",
    "            dropout=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "latent = torch.rand(4,5,3)\n",
    "action = torch.rand(4,5,1)\n",
    "temporal_mask = get_subsequent_mask(latent)\n",
    "print(temporal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     dist_feat = storm_transformer(latent, action, temporal_mask)\n",
    "storm_transformer.eval()\n",
    "dist_feat, kv_test = storm_transformer.forward_context(latent, action, temporal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     storm_transformer.reset_kv_cache_list(4,torch.float32)\n",
    "#     test_list = []\n",
    "#     for i in range(3):\n",
    "#         dist_feat_kv = storm_transformer.forward_with_kv_cache(latent[:,i:i+1,:], action[:,i:i+1,:],i)\n",
    "#         test_list.append(dist_feat_kv)\n",
    "storm_transformer.reset_kv_cache_list(4,torch.float32)\n",
    "test_list = []\n",
    "for i in range(5):\n",
    "    dist_feat_kv = storm_transformer.forward_with_kv_cache(latent[:,i:i+1,:], action[:,i:i+1,:],i)\n",
    "    test_list.append(dist_feat_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3835, -0.4380, -0.9455],\n",
       "         [ 0.9117,  0.4804, -1.3921],\n",
       "         [ 1.4132, -0.6600, -0.7532],\n",
       "         [ 0.6356,  0.7763, -1.4119],\n",
       "         [ 0.9831,  0.3889, -1.3720]],\n",
       "\n",
       "        [[ 1.3835, -0.4380, -0.9455],\n",
       "         [ 1.2935, -1.1419, -0.1516],\n",
       "         [ 1.4102, -0.6135, -0.7968],\n",
       "         [ 0.4802,  0.9119, -1.3921],\n",
       "         [ 0.8993,  0.4956, -1.3949]],\n",
       "\n",
       "        [[ 1.3835, -0.4380, -0.9455],\n",
       "         [ 0.9117,  0.4804, -1.3921],\n",
       "         [ 1.4132, -0.6600, -0.7532],\n",
       "         [ 0.6356,  0.7763, -1.4119],\n",
       "         [ 0.9831,  0.3889, -1.3720]],\n",
       "\n",
       "        [[ 1.3835, -0.4380, -0.9455],\n",
       "         [ 1.2362, -0.0232, -1.2130],\n",
       "         [ 1.4136, -0.6722, -0.7414],\n",
       "         [ 0.6218,  0.7891, -1.4109],\n",
       "         [ 0.9802,  0.3927, -1.3729]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_feat[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3835, -0.4380, -0.9455],\n",
       "         [ 0.9117,  0.4804, -1.3921],\n",
       "         [ 1.4132, -0.6600, -0.7532],\n",
       "         [ 0.6356,  0.7763, -1.4119],\n",
       "         [ 0.9831,  0.3889, -1.3720]],\n",
       "\n",
       "        [[ 1.3835, -0.4380, -0.9455],\n",
       "         [ 1.2935, -1.1419, -0.1516],\n",
       "         [ 1.4102, -0.6135, -0.7968],\n",
       "         [ 0.4802,  0.9119, -1.3921],\n",
       "         [ 0.8993,  0.4956, -1.3949]],\n",
       "\n",
       "        [[ 1.3835, -0.4380, -0.9455],\n",
       "         [ 0.9117,  0.4804, -1.3921],\n",
       "         [ 1.4132, -0.6600, -0.7532],\n",
       "         [ 0.6356,  0.7763, -1.4119],\n",
       "         [ 0.9831,  0.3889, -1.3720]],\n",
       "\n",
       "        [[ 1.3835, -0.4380, -0.9455],\n",
       "         [ 1.2362, -0.0232, -1.2130],\n",
       "         [ 1.4136, -0.6722, -0.7414],\n",
       "         [ 0.6218,  0.7891, -1.4109],\n",
       "         [ 0.9802,  0.3927, -1.3729]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.cat(test_list,dim=1)\n",
    "result[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 1.1871,  0.0721, -1.2592],\n",
       "          [ 0.4294,  0.9522, -1.3816],\n",
       "          [ 1.3325, -0.2559, -1.0766],\n",
       "          [ 0.1741,  1.1284, -1.3025],\n",
       "          [ 0.5249,  0.8748, -1.3997]],\n",
       " \n",
       "         [[ 1.1871,  0.0721, -1.2592],\n",
       "          [ 1.3714, -0.9846, -0.3868],\n",
       "          [ 1.3325, -0.2559, -1.0766],\n",
       "          [ 0.1741,  1.1284, -1.3025],\n",
       "          [ 0.5249,  0.8748, -1.3997]],\n",
       " \n",
       "         [[ 1.1871,  0.0721, -1.2592],\n",
       "          [ 0.4294,  0.9522, -1.3816],\n",
       "          [ 1.3325, -0.2559, -1.0766],\n",
       "          [ 0.1741,  1.1284, -1.3025],\n",
       "          [ 0.5249,  0.8748, -1.3997]],\n",
       " \n",
       "         [[ 1.1871,  0.0721, -1.2592],\n",
       "          [ 0.8712,  0.5291, -1.4004],\n",
       "          [ 1.3325, -0.2559, -1.0766],\n",
       "          [ 0.1741,  1.1284, -1.3025],\n",
       "          [ 0.5249,  0.8748, -1.3997]]], grad_fn=<CatBackward0>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_transformer.kv_cache_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1871,  0.0721, -1.2592],\n",
       "         [ 0.4294,  0.9522, -1.3816],\n",
       "         [ 1.3325, -0.2559, -1.0766],\n",
       "         [ 0.1741,  1.1284, -1.3025],\n",
       "         [ 0.5249,  0.8748, -1.3997]],\n",
       "\n",
       "        [[ 1.1871,  0.0721, -1.2592],\n",
       "         [ 1.3714, -0.9846, -0.3868],\n",
       "         [ 1.3325, -0.2559, -1.0766],\n",
       "         [ 0.1741,  1.1284, -1.3025],\n",
       "         [ 0.5249,  0.8748, -1.3997]],\n",
       "\n",
       "        [[ 1.1871,  0.0721, -1.2592],\n",
       "         [ 0.4294,  0.9522, -1.3816],\n",
       "         [ 1.3325, -0.2559, -1.0766],\n",
       "         [ 0.1741,  1.1284, -1.3025],\n",
       "         [ 0.5249,  0.8748, -1.3997]],\n",
       "\n",
       "        [[ 1.1871,  0.0721, -1.2592],\n",
       "         [ 0.8712,  0.5291, -1.4004],\n",
       "         [ 1.3325, -0.2559, -1.0766],\n",
       "         [ 0.1741,  1.1284, -1.3025],\n",
       "         [ 0.5249,  0.8748, -1.3997]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_test[:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unitree-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
